{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Apache Iceberg with PySpark – End-to-End Hands-on\n",
    "\n",
    "This notebook demonstrates **Apache Iceberg** using **PySpark** on a local Windows setup.\n",
    "\n",
    "### What you will learn\n",
    "- Create Iceberg tables\n",
    "- Append data safely\n",
    "- Schema evolution (add columns)\n",
    "- Accidental overwrite & recovery\n",
    "- Time travel using snapshots\n",
    "\n",
    "⚠️ **Important:** Iceberg manages schema evolution internally. **No `mergeSchema=true` is required**, unlike plain Parquet.\n"
   ],
   "id": "8964fc672e8b356d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-29T11:05:40.363420Z",
     "start_time": "2025-12-29T11:05:40.169333Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 3.5.3 paths\n",
    "spark_home = r\"C:\\spark\\spark-3.5.3-bin-hadoop3\"\n",
    "sys.path.insert(0, spark_home + r\"\\python\")\n",
    "sys.path.insert(0, spark_home + r\"\\python\\lib\\py4j-0.10.9.7-src.zip\")\n",
    "\n",
    "# Python executables\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\Raghava\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\Raghava\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:05:51.865423Z",
     "start_time": "2025-12-29T11:05:42.615059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergLocalSetup-Spark353\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.local\",\n",
    "        \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.local.warehouse\",\n",
    "        \"file:///H:/spark_practice/with_Iceberg/warehouse\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "id": "b4634b38cb9e204d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 1 – Create Iceberg Table & Initial Load",
   "id": "1d53fc0101907512"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:05:56.730199Z",
     "start_time": "2025-12-29T11:05:51.881061Z"
    }
   },
   "cell_type": "code",
   "source": "spark.sql(\"SHOW CATALOGS\").show()\n",
   "id": "f1170df5019a837f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:07:06.273463Z",
     "start_time": "2025-12-29T11:07:05.059809Z"
    }
   },
   "cell_type": "code",
   "source": "spark.sql(\"CREATE DATABASE IF NOT EXISTS local.testdb\")",
   "id": "aad017f28f4139a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:11:55.416483Z",
     "start_time": "2025-12-29T11:11:46.882811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_day1 = spark.createDataFrame(\n",
    "    [(1,'Ramesh'),(2,'Pavan')],\n",
    "    [\"id\", \"name\"]\n",
    ")\n",
    "\n",
    "df_day1.writeTo(\"local.testdb.users\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .create()\n"
   ],
   "id": "bfed565360cab367",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:22:24.343205Z",
     "start_time": "2025-12-29T11:22:22.612803Z"
    }
   },
   "cell_type": "code",
   "source": "spark.table(\"local.testdb.users\").show()\n",
   "id": "88eae0bb16ea4771",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Ramesh|\n",
      "|  2| Pavan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:26:14.697826Z",
     "start_time": "2025-12-29T11:26:14.247072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE local.testdb.users\n",
    "ADD COLUMN age INT\n",
    "\"\"\")\n"
   ],
   "id": "5f52029cd9c3d168",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Day 2 – Schema Evolution (Add Column) & Append Data\n",
    "\n",
    "Notice that we **do not use `mergeSchema=true`**. Iceberg evolves schema safely at metadata level."
   ],
   "id": "a0f03a22cc3a3762"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:26:20.902140Z",
     "start_time": "2025-12-29T11:26:20.824009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_day2 = spark.createDataFrame(\n",
    "    [(3, \"Akhil\", 35), (4, \"Nikhil\", 28)],\n",
    "    [\"id\", \"name\", \"age\"]\n",
    ")\n"
   ],
   "id": "b35fd2cb51195f27",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:26:30.502419Z",
     "start_time": "2025-12-29T11:26:23.778146Z"
    }
   },
   "cell_type": "code",
   "source": "df_day2.writeTo(\"local.testdb.users\").append()\n",
   "id": "b8291724049cd323",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:27:21.713104Z",
     "start_time": "2025-12-29T11:27:21.074374Z"
    }
   },
   "cell_type": "code",
   "source": "spark.table(\"local.testdb.users\").show()\n",
   "id": "21f88f5d7bd58138",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|  name| age|\n",
      "+---+------+----+\n",
      "|  1|Ramesh|NULL|\n",
      "|  3| Akhil|  35|\n",
      "|  2| Pavan|NULL|\n",
      "|  4|Nikhil|  28|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 3 – Accidental Overwrite (Simulating a Production Mistake)",
   "id": "d1fc3f9fc714b4ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:31:34.492873Z",
     "start_time": "2025-12-29T11:31:29.973738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_bad = spark.createDataFrame(\n",
    "    [(99, \"Hacker\", 100)],\n",
    "    [\"id\", \"name\", \"age\"]\n",
    ")\n",
    "\n",
    "df_bad.writeTo(\"local.testdb.users\").overwritePartitions()\n",
    "\n"
   ],
   "id": "abf33db9e76ce069",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:32:35.794333Z",
     "start_time": "2025-12-29T11:32:35.423172Z"
    }
   },
   "cell_type": "code",
   "source": "spark.table(\"local.testdb.users\").show()\n",
   "id": "3ce84d1a8a782923",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "| 99|Hacker|100|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Day 4 – Time Travel & Recovery\n",
    "\n",
    "Iceberg keeps all snapshots. We can query or rollback safely."
   ],
   "id": "7a93540499a6b901"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:32:57.278029Z",
     "start_time": "2025-12-29T11:32:55.827898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation\n",
    "FROM local.testdb.users.snapshots\n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n"
   ],
   "id": "24e36d5f8ef63ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5193484984745520434|2025-12-29 16:41:55.26 |append   |\n",
      "|3451235462764937396|2025-12-29 16:56:29.823|append   |\n",
      "|2047872761209547268|2025-12-29 17:01:34.409|overwrite|\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:33:25.124622Z",
     "start_time": "2025-12-29T11:33:24.700147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snapshot_id = 3451235462764937396\n",
    "\n",
    "spark.read \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", snapshot_id) \\\n",
    "    .load(\"local.testdb.users\") \\\n",
    "    .show()\n"
   ],
   "id": "87630bb5f4f173a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|  name| age|\n",
      "+---+------+----+\n",
      "|  1|Ramesh|NULL|\n",
      "|  2| Pavan|NULL|\n",
      "|  3| Akhil|  35|\n",
      "|  4|Nikhil|  28|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:36:20.154492Z",
     "start_time": "2025-12-29T11:36:19.426626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.sql(f\"\"\"\n",
    "CALL local.system.rollback_to_snapshot(\n",
    "  'testdb.users',\n",
    "  {snapshot_id}\n",
    ")\n",
    "\"\"\")\n"
   ],
   "id": "1b937a9f6e74d59b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T11:36:24.040800Z",
     "start_time": "2025-12-29T11:36:23.441571Z"
    }
   },
   "cell_type": "code",
   "source": "spark.table(\"local.testdb.users\").show()\n",
   "id": "d01fff662d84bbb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|  name| age|\n",
      "+---+------+----+\n",
      "|  1|Ramesh|NULL|\n",
      "|  2| Pavan|NULL|\n",
      "|  3| Akhil|  35|\n",
      "|  4|Nikhil|  28|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Key Takeaways\n",
    "- Iceberg prevents data loss using snapshots\n",
    "- Schema evolution is **automatic & safe**\n",
    "- **No `mergeSchema=true` required**\n",
    "- Always read data via table, not raw parquet files\n"
   ],
   "id": "64be8d66cffb28d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
