{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93e596f",
   "metadata": {},
   "source": [
    "# Life Without Apache Iceberg ‚ùÑÔ∏è (Windows + Local FS Demo)\n",
    "\n",
    "This notebook demonstrates **what goes wrong when we manage a data lake using only Parquet files**, without a table format like **Apache Iceberg**.\n",
    "\n",
    "Environment:\n",
    "- Spark **3.5.3**\n",
    "- Windows local filesystem\n",
    "- PySpark\n",
    "\n",
    "This notebook is structured for:\n",
    "- üìå LinkedIn technical posts\n",
    "- üìÅ Local experimentation\n",
    "- üß† GitHub documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22172d9b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Spark Setup (Windows Local)"
   ]
  },
  {
   "cell_type": "code",
   "id": "86cfd692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:05:58.462348Z",
     "start_time": "2025-12-29T06:05:58.422320Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Spark 3.5.3 paths\n",
    "spark_home = r\"C:\\spark\\spark-3.5.3-bin-hadoop3\"\n",
    "sys.path.insert(0, spark_home + r\"\\python\")\n",
    "sys.path.insert(0, spark_home + r\"\\python\\lib\\py4j-0.10.9.7-src.zip\")\n",
    "\n",
    "# Python executables\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\Raghava\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\Raghava\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "97bc9ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:08.546092Z",
     "start_time": "2025-12-29T06:05:59.571682Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"withoutIceberg\").getOrCreate()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "82e46cc0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Day 1 ‚Äì Initial Parquet Write"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0b7170d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:20.904500Z",
     "start_time": "2025-12-29T06:06:08.546092Z"
    }
   },
   "source": [
    "data_day1 = [(1,'Ramesh'),(2,'Pavan')]\n",
    "df_day1 = spark.createDataFrame(data_day1,['id', 'name'])\n",
    "df_day1.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Ramesh|\n",
      "|  2| Pavan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "f5aa9935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:26.613709Z",
     "start_time": "2025-12-29T06:06:20.941412Z"
    }
   },
   "source": [
    "df_day1.write.mode(\"overwrite\").parquet(\n",
    "    \"file:///H:/spark_practice/iceberg_poc/without_iceberg\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:28.083909Z",
     "start_time": "2025-12-29T06:06:26.653736Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.parquet('without_iceberg/part-00001-30db33eb-eeb6-4e95-89ad-3e48b4e96778-c000.snappy.parquet')",
   "id": "1881b75c427f58e0",
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/H:/spark_practice/iceberg_poc/without_iceberg/part-00001-30db33eb-eeb6-4e95-89ad-3e48b4e96778-c000.snappy.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwithout_iceberg/part-00001-30db33eb-eeb6-4e95-89ad-3e48b4e96778-c000.snappy.parquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:544\u001B[0m, in \u001B[0;36mDataFrameReader.parquet\u001B[1;34m(self, *paths, **options)\u001B[0m\n\u001B[0;32m    533\u001B[0m int96RebaseMode \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint96RebaseMode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    534\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[0;32m    535\u001B[0m     mergeSchema\u001B[38;5;241m=\u001B[39mmergeSchema,\n\u001B[0;32m    536\u001B[0m     pathGlobFilter\u001B[38;5;241m=\u001B[39mpathGlobFilter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    541\u001B[0m     int96RebaseMode\u001B[38;5;241m=\u001B[39mint96RebaseMode,\n\u001B[0;32m    542\u001B[0m )\n\u001B[1;32m--> 544\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mC:\\spark\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: file:/H:/spark_practice/iceberg_poc/without_iceberg/part-00001-30db33eb-eeb6-4e95-89ad-3e48b4e96778-c000.snappy.parquet."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:28.119744100Z",
     "start_time": "2025-12-29T05:12:20.820416Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "4e1ce67fcf3987bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:28.148597Z",
     "start_time": "2025-12-29T05:13:21.058286Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.parquet('without_iceberg/part-00003-30db33eb-eeb6-4e95-89ad-3e48b4e96778-c000.snappy.parquet')",
   "id": "b80193eaac306e2b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:28.156861Z",
     "start_time": "2025-12-29T05:13:23.977012Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "df55f8f36089ee66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  2|Pavan|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "ea0c55b2",
   "metadata": {},
   "source": [
    "**What happens here?**\n",
    "\n",
    "- Spark writes raw Parquet files\n",
    "- Each partition becomes a physical file\n",
    "- No table metadata, no schema enforcement\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e556b9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:28.164865900Z",
     "start_time": "2025-12-29T05:15:53.948573Z"
    }
   },
   "source": [
    "df_day1.rdd.getNumPartitions()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b10fa1be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:29.704129Z",
     "start_time": "2025-12-29T06:06:29.670237Z"
    }
   },
   "source": [
    "df_day1.explain(True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string\n",
      "LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[id#0L,name#1]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìÇ Day 1 ‚Äì Initial Parquet Write (Physical Files)\n",
    "\n",
    "Below image shows multiple Parquet files created after the first write:\n",
    "\n",
    "![Day 1 Parquet Files](images/day1_initial_write.png)\n"
   ],
   "id": "6ef54be6be3a7107"
  },
  {
   "cell_type": "markdown",
   "id": "7fb057db",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Day 2 ‚Äì Append With Schema Drift"
   ]
  },
  {
   "cell_type": "code",
   "id": "db3c99c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:44.469894Z",
     "start_time": "2025-12-29T06:06:44.300156Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "data_day2 = [(3,'Akhil'),(4,'Nikhil')]\n",
    "df_day2 = spark.createDataFrame(data_day2,['id','name']) \\\n",
    "    .withColumn(\"updated_at\", current_timestamp())\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1734d518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:06:53.506535Z",
     "start_time": "2025-12-29T06:06:48.108408Z"
    }
   },
   "source": [
    "df_day2.write.mode('append').parquet(\n",
    "    \"file:///H:/spark_practice/iceberg_poc/without_iceberg\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:08:06.480611Z",
     "start_time": "2025-12-29T06:08:06.291694Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.parquet(\"without_iceberg/part-00003-382da9d3-de2f-4623-a792-0a593ebbd6c1-c000.snappy.parquet\")",
   "id": "aa416ed323c307d0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:08:24.951390Z",
     "start_time": "2025-12-29T06:08:24.204829Z"
    }
   },
   "cell_type": "code",
   "source": "df.show(truncate = False)",
   "id": "a4a71819c2191067",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------------+\n",
      "|id |name  |updated_at                |\n",
      "+---+------+--------------------------+\n",
      "|4  |Nikhil|2025-12-29 11:36:48.202634|\n",
      "+---+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚ö†Ô∏è Day 2 ‚Äì Schema Drift in Action\n",
    "\n",
    "Some Parquet files now contain `updated_at`, others don‚Äôt:\n",
    "\n",
    "![Day 2 Schema Drift](images/day2_schema_drift.png)\n"
   ],
   "id": "4d04f4c37de11b75"
  },
  {
   "cell_type": "markdown",
   "id": "7dd7b0dd",
   "metadata": {},
   "source": [
    "### üö® Schema Drift Problem\n",
    "\n",
    "- Old Parquet files **do not have `updated_at`**\n",
    "- New Parquet files **do have it**\n",
    "- Dataset now has **multiple schemas**\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "56856894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:28:37.081926Z",
     "start_time": "2025-12-29T05:28:36.324251Z"
    }
   },
   "source": [
    "df = spark.read \\\n",
    "    .option(\"mergeSchema\", \"false\") \\\n",
    "    .parquet(\"without_iceberg/\")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  4|Nikhil|\n",
      "|  3| Akhil|\n",
      "|  1|Ramesh|\n",
      "|  2| Pavan|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:09:40.172803Z",
     "start_time": "2025-12-29T06:09:39.525569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .parquet(\"without_iceberg/\")\n",
    "\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n"
   ],
   "id": "c11891fe99134c38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------------+\n",
      "|id |name  |updated_at                |\n",
      "+---+------+--------------------------+\n",
      "|4  |Nikhil|2025-12-29 11:36:48.202634|\n",
      "|3  |Akhil |2025-12-29 11:36:48.202634|\n",
      "|1  |Ramesh|NULL                      |\n",
      "|2  |Pavan |NULL                      |\n",
      "+---+------+--------------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "ff1f08bc",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "- Spark ignores the new column\n",
    "- Data appears incomplete\n",
    "- Manual schema handling required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75917700",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Day 3 ‚Äì Accidental Overwrite (Data Loss)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2ec094c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T06:09:59.515122Z",
     "start_time": "2025-12-29T06:09:50.748284Z"
    }
   },
   "source": [
    "df_bad = spark.createDataFrame([(999,'Malicious')],['id','name'])\n",
    "df_bad.write.mode('overwrite').parquet(\n",
    "    \"file:///H:/spark_practice/iceberg_poc/without_iceberg\"\n",
    ")\n",
    "df_bad.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|999|Malicious|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚ùå Day 3 ‚Äì Accidental Overwrite\n",
    "\n",
    "One overwrite wiped the entire dataset:\n",
    "\n",
    "![Day 3 Overwrite](images/day3_overwrite.png)\n"
   ],
   "id": "57461d1a235adb9d"
  },
  {
   "cell_type": "markdown",
   "id": "b0b5a834",
   "metadata": {},
   "source": [
    "### ‚ùå Final Outcome\n",
    "\n",
    "- Entire dataset is deleted\n",
    "- No rollback\n",
    "- No time travel\n",
    "- No ACID guarantees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db39529",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "| Feature | Parquet Only |\n",
    "|-------|-------------|\n",
    "| Schema evolution | ‚ùå Manual |\n",
    "| Append safety | ‚ùå Risky |\n",
    "| Time travel | ‚ùå No |\n",
    "| Rollback | ‚ùå No |\n",
    "| ACID guarantees | ‚ùå No |\n",
    "\n",
    "**This is exactly why Apache Iceberg exists.** ‚ùÑÔ∏è"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
