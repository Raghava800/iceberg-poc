{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93e596f",
   "metadata": {},
   "source": [
    "# Life Without Apache Iceberg ‚ùÑÔ∏è (Windows + Local FS Demo)\n",
    "\n",
    "This notebook demonstrates **what goes wrong when we manage a data lake using only Parquet files**, without a table format like **Apache Iceberg**.\n",
    "\n",
    "Environment:\n",
    "- Spark **3.5.3**\n",
    "- Windows local filesystem\n",
    "- PySpark\n",
    "\n",
    "This notebook is structured for:\n",
    "- üìå LinkedIn technical posts\n",
    "- üìÅ Local experimentation\n",
    "- üß† GitHub documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22172d9b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Spark Setup (Windows Local)"
   ]
  },
  {
   "cell_type": "code",
   "id": "86cfd692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:09:56.120553Z",
     "start_time": "2025-12-29T05:09:56.089298Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Spark 3.5.3 paths\n",
    "spark_home = r\"C:\\spark\\spark-3.5.3-bin-hadoop3\"\n",
    "sys.path.insert(0, spark_home + r\"\\python\")\n",
    "sys.path.insert(0, spark_home + r\"\\python\\lib\\py4j-0.10.9.7-src.zip\")\n",
    "\n",
    "# Python executables\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\Raghava\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\Raghava\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "97bc9ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:10:05.370957Z",
     "start_time": "2025-12-29T05:09:56.151808Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"withoutIceberg\").getOrCreate()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "82e46cc0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Day 1 ‚Äì Initial Parquet Write"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0b7170d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:10:22.621078Z",
     "start_time": "2025-12-29T05:10:09.574055Z"
    }
   },
   "source": [
    "data_day1 = [(1,'Ramesh'),(2,'Pavan')]\n",
    "df_day1 = spark.createDataFrame(data_day1,['id', 'name'])\n",
    "df_day1.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Ramesh|\n",
      "|  2| Pavan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "f5aa9935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:10:28.596587Z",
     "start_time": "2025-12-29T05:10:22.729702Z"
    }
   },
   "source": [
    "df_day1.write.mode(\"overwrite\").parquet(\n",
    "    \"file:///H:/spark_practice/iceberg_poc/without_iceberg\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:12:18.372203Z",
     "start_time": "2025-12-29T05:12:17.601568Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.parquet('without_iceberg/part-00001-30db33eb-eeb6-4e95-89ad-3e48b4e96778-c000.snappy.parquet')",
   "id": "1881b75c427f58e0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:12:21.994205Z",
     "start_time": "2025-12-29T05:12:20.820416Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "4e1ce67fcf3987bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:13:21.714415Z",
     "start_time": "2025-12-29T05:13:21.058286Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.parquet('without_iceberg/part-00003-30db33eb-eeb6-4e95-89ad-3e48b4e96778-c000.snappy.parquet')",
   "id": "b80193eaac306e2b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:13:24.272800Z",
     "start_time": "2025-12-29T05:13:23.977012Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "df55f8f36089ee66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  2|Pavan|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "ea0c55b2",
   "metadata": {},
   "source": [
    "**What happens here?**\n",
    "\n",
    "- Spark writes raw Parquet files\n",
    "- Each partition becomes a physical file\n",
    "- No table metadata, no schema enforcement\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e556b9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:15:54.053989Z",
     "start_time": "2025-12-29T05:15:53.948573Z"
    }
   },
   "source": [
    "df_day1.rdd.getNumPartitions()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b10fa1be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:15:56.014444Z",
     "start_time": "2025-12-29T05:15:55.983172Z"
    }
   },
   "source": [
    "df_day1.explain(True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string\n",
      "LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[id#0L,name#1]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìÇ Day 1 ‚Äì Initial Parquet Write (Physical Files)\n",
    "\n",
    "Below image shows multiple Parquet files created after the first write:\n",
    "\n",
    "![Day 1 Parquet Files](images/day1_initial_write.png)\n"
   ],
   "id": "6ef54be6be3a7107"
  },
  {
   "cell_type": "markdown",
   "id": "7fb057db",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Day 2 ‚Äì Append With Schema Drift"
   ]
  },
  {
   "cell_type": "code",
   "id": "db3c99c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:22:38.708718Z",
     "start_time": "2025-12-29T05:22:38.513636Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "data_day2 = [(3,'Akhil'),(4,'Nikhil')]\n",
    "df_day2 = spark.createDataFrame(data_day2,['id','name']) \\\n",
    "    .withColumn(\"updated_at\", current_timestamp())\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "1734d518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:29:34.333380Z",
     "start_time": "2025-12-29T05:29:29.777423Z"
    }
   },
   "source": [
    "df_day2.write.mode('append').parquet(\n",
    "    \"file:///H:/spark_practice/iceberg_poc/without_iceberg\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:30:44.684348Z",
     "start_time": "2025-12-29T05:30:43.995812Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.parquet(\"without_iceberg/part-00003-519d897e-4ff0-4f2a-a7b3-8ca5ed55bf8d-c000.snappy.parquet\")",
   "id": "aa416ed323c307d0",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:30:46.456995Z",
     "start_time": "2025-12-29T05:30:46.007797Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "a4a71819c2191067",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n",
      "| id|  name|          updated_at|\n",
      "+---+------+--------------------+\n",
      "|  4|Nikhil|2025-12-29 10:52:...|\n",
      "+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚ö†Ô∏è Day 2 ‚Äì Schema Drift in Action\n",
    "\n",
    "Some Parquet files now contain `updated_at`, others don‚Äôt:\n",
    "\n",
    "![Day 2 Schema Drift](images/day2_schema_drift.png)\n"
   ],
   "id": "4d04f4c37de11b75"
  },
  {
   "cell_type": "markdown",
   "id": "7dd7b0dd",
   "metadata": {},
   "source": [
    "### üö® Schema Drift Problem\n",
    "\n",
    "- Old Parquet files **do not have `updated_at`**\n",
    "- New Parquet files **do have it**\n",
    "- Dataset now has **multiple schemas**\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "56856894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:28:37.081926Z",
     "start_time": "2025-12-29T05:28:36.324251Z"
    }
   },
   "source": [
    "df = spark.read \\\n",
    "    .option(\"mergeSchema\", \"false\") \\\n",
    "    .parquet(\"without_iceberg/\")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  4|Nikhil|\n",
      "|  3| Akhil|\n",
      "|  1|Ramesh|\n",
      "|  2| Pavan|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "ff1f08bc",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "- Spark ignores the new column\n",
    "- Data appears incomplete\n",
    "- Manual schema handling required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75917700",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Day 3 ‚Äì Accidental Overwrite (Data Loss)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2ec094c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T05:33:21.456573Z",
     "start_time": "2025-12-29T05:33:11.743962Z"
    }
   },
   "source": [
    "df_bad = spark.createDataFrame([(999,'Malicious')],['id','name'])\n",
    "df_bad.write.mode('overwrite').parquet(\n",
    "    \"file:///H:/spark_practice/iceberg_poc/without_iceberg\"\n",
    ")\n",
    "df_bad.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|999|Malicious|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚ùå Day 3 ‚Äì Accidental Overwrite\n",
    "\n",
    "One overwrite wiped the entire dataset:\n",
    "\n",
    "![Day 3 Overwrite](images/day3_overwrite.png)\n"
   ],
   "id": "57461d1a235adb9d"
  },
  {
   "cell_type": "markdown",
   "id": "b0b5a834",
   "metadata": {},
   "source": [
    "### ‚ùå Final Outcome\n",
    "\n",
    "- Entire dataset is deleted\n",
    "- No rollback\n",
    "- No time travel\n",
    "- No ACID guarantees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db39529",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "| Feature | Parquet Only |\n",
    "|-------|-------------|\n",
    "| Schema evolution | ‚ùå Manual |\n",
    "| Append safety | ‚ùå Risky |\n",
    "| Time travel | ‚ùå No |\n",
    "| Rollback | ‚ùå No |\n",
    "| ACID guarantees | ‚ùå No |\n",
    "\n",
    "**This is exactly why Apache Iceberg exists.** ‚ùÑÔ∏è"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
